# گزارش کار پروژه خوشه‌بندی — K-means و HAC روی Customer Marketing

**درس:** یادگیری ماشین  
**موضوع:** خوشه‌بندی (K-means و HAC)  
**دیتاست:** Customer Marketing

---

## فهرست مطالب

1. [چکیده](#۱-چکیده)
2. [مقدمه و هدف پروژه](#۲-مقدمه-و-هدف-پروژه)
3. [توصیف مجموعه‌داده](#۳-توصیف-مجموعه‌داده)
4. [پیش‌پردازش و انتخاب ویژگی](#۴-پیش‌پردازش-و-انتخاب-ویژگی)
5. [الگوریتم‌های خوشه‌بندی](#۵-الگوریتمهای-خوشه‌بندی)
6. [معیارها و نتایج](#۶-معیارها-و-نتایج)
7. [تحلیل و مقایسه روش‌ها](#۷-تحلیل-و-مقایسه-روشها)
8. [نتیجه‌گیری](#۸-نتیجه‌گیری)
9. [فایل‌های پروژه](#۹-فایلهای-پروژه)

---

## ۱. چکیده

در این پروژه با استفاده از مجموعه‌داده **Customer Marketing**، رفتار مشتریان با دو الگوریتم خوشه‌بندی **K-means** و **خوشه‌بندی سلسله‌مراتبی تجمعی (HAC)** تحلیل شده است. پس از انتخاب تعدادی ویژگی عددی مرتبط با الگوی خرید و پیش‌پردازش آن‌ها، الگوریتم K-means برای مقادیر مختلف K اجرا و تابع هزینه (SSE) و سرعت همگرایی بررسی شده است. سپس HAC با دو نوع لینک **نزدیک‌ترین زوج (Single Linkage)** و **دورترین زوج (Complete Linkage)** پیاده‌سازی و تابع هزینه آن برای تعداد خوشه‌های مختلف محاسبه شده است. در نهایت، منحنی‌های SSE سه روش روی یک نمودار مقایسه شده‌اند تا رفتار هر الگوریتم نسبت به K و تفاوت ساختار خوشه‌ها بررسی شود.

---

## ۲. مقدمه و هدف پروژه

هدف این گزارش، تحلیل خوشه‌بندی مشتریان بر اساس رفتار خرید است تا بتوان الگوهای نهفته در داده را کشف و از آن برای تصمیم‌سازی بازاریابی استفاده کرد. به طور خاص:

- پیاده‌سازی الگوریتم **K-means** با مقداردهی اولیه تصادفی مراکز.
- بررسی **سرعت همگرایی** و انتخاب K مناسب با استفاده از نمودار SSE.
- پیاده‌سازی الگوریتم **HAC** با دو روش لینک:
  - نزدیک‌ترین زوج (Single Linkage)
  - دورترین زوج (Complete Linkage)
- رسم و مقایسهٔ تابع هزینه (SSE) هر دو الگوریتم برای Kهای مختلف.

پروژه در محیط **Jupyter Notebook** پیاده‌سازی شده و از **نمودارها** و **نمونه‌کدها** در این گزارش استفاده شده است.

---

## ۳. توصیف مجموعه‌داده

### ۳.۱ مشخصات کلی

فایل داده در مسیر زیر قرار دارد:

- `data/Customer marketing.csv`

این مجموعه‌داده شامل اطلاعات جمعیت‌شناختی و رفتاری مشتریان یک کمپین بازاریابی است؛ از جمله:

- سال تولد، سطح تحصیلات، وضعیت تأهل
- درآمد سالانه
- میزان خرید در دسته‌های مختلف محصول (شراب، گوشت، ماهی، شیرینی، طلا و ...)
- تعداد خریدهای انجام‌شده از طریق کانال‌های مختلف (وب، کاتالوگ، فروشگاه فیزیکی)
- تعداد بازدید از وب‌سایت در ماه

### ۳.۲ ویژگی‌های عددی استفاده‌شده

برای خوشه‌بندی، تعدادی ویژگی عددی مرتبط انتخاب شده‌اند؛ از جمله:

- `Income`
- `MntWines`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, `MntGoldProds`
- `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `NumStorePurchases`
- `NumWebVisitsMonth`

این ویژگی‌ها رفتار خرید و میزان درگیر بودن مشتری با فروشگاه را توصیف می‌کنند و برای ساخت خوشه‌های معنادار مناسب هستند.

---

## ۴. پیش‌پردازش و انتخاب ویژگی

### ۴.۱ مدیریت مقادیر گمشده و نرمال‌سازی

در ابتدا تنها ستون‌های عددی مورد نیاز انتخاب شده و سطرهایی که در این ستون‌ها دارای مقادیر گمشده بودند حذف شده‌اند. سپس برای جلوگیری از تسلط یک ویژگی روی فاصله اقلیدسی، همهٔ ویژگی‌ها با نرمال‌سازی استاندارد (میانگین صفر و انحراف معیار یک) مقیاس‌بندی شده‌اند.

نمونه کد پیش‌پردازش:

```python
numeric_features = [
    "Income",
    "MntWines",
    "MntFruits",
    "MntMeatProducts",
    "MntFishProducts",
    "MntSweetProducts",
    "MntGoldProds",
    "NumDealsPurchases",
    "NumWebPurchases",
    "NumCatalogPurchases",
    "NumStorePurchases",
    "NumWebVisitsMonth",
]

df_numeric = df[numeric_features].dropna()
X = df_numeric.values.astype(float)

X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_std[X_std == 0] = 1.0
X_scaled = (X - X_mean) / X_std
```

---

## ۵. الگوریتم‌های خوشه‌بندی

### ۵.۱ الگوریتم K-means

#### ۵.۱.۱ ایدهٔ اصلی

در الگوریتم K-means داده‌ها به K خوشه تقسیم می‌شوند به طوری که مجموع مربعات فاصلهٔ نقاط از مرکز خوشه‌شان (**SSE**) حداقل شود. روند کلی:

- انتخاب تصادفی K مرکز اولیه از بین نمونه‌ها.
- تخصیص هر نقطه به نزدیک‌ترین مرکز بر اساس فاصله اقلیدسی.
- به‌روزرسانی مراکز با میانگین نقاط هر خوشه.
- تکرار مراحل فوق تا زمانی که جابجایی مراکز کوچک شود یا به حداکثر تعداد تکرار برسیم.

#### ۵.۱.۲ پیاده‌سازی

نمونه‌ای از پیاده‌سازی تابع `kmeans`:

```python
def kmeans(X, k, max_iters=100, tol=1e-4, random_state=None):
    rng = np.random.default_rng(random_state)
    n_samples = X.shape[0]
    indices = rng.choice(n_samples, size=k, replace=False)
    centroids = X[indices].copy()

    for it in range(max_iters):
        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)
        labels = np.argmin(distances, axis=1)

        new_centroids = np.zeros_like(centroids)
        for j in range(k):
            mask = labels == j
            if np.any(mask):
                new_centroids[j] = X[mask].mean(axis=0)
            else:
                new_centroids[j] = X[rng.integers(0, n_samples)]

        shift = np.linalg.norm(new_centroids - centroids)
        centroids = new_centroids
        if shift < tol:
            break

    distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)
    labels = np.argmin(distances, axis=1)
    sse = np.sum((X - centroids[labels]) ** 2)

    return labels, centroids, sse, it + 1
```

خروجی این تابع شامل برچسب خوشهٔ هر نمونه (`labels`)، مراکز نهایی (`centroids`)، مقدار تابع هزینه (`sse`) و تعداد تکرار تا همگرایی (`it + 1`) است.

#### ۵.۱.۳ بررسی سرعت همگرایی و انتخاب K

برای تحلیل رفتار K-means نسبت به تعداد خوشه‌ها، الگوریتم برای چند مقدار مختلف K (مثلاً ۲ تا ۱۰) و چند مقداردهی اولیه تصادفی اجرا شده است. برای هر K:

- کمترین مقدار SSE در بین اجراها ثبت شده است.
- میانگین تعداد تکرار تا همگرایی محاسبه شده است.

**نمودار ۱ — SSE و سرعت همگرایی K-means**

![K-means SSE and convergence](images/01.png)

از نمودار SSE بر حسب K می‌توان با روش «آرنج» مقدار مناسبی برای K انتخاب کرد؛ جایی که پس از آن کاهش SSE کند می‌شود و تقسیم بیشتر خوشه‌ها توجیه زیادی ندارد.

### ۵.۲ الگوریتم HAC (خوشه‌بندی سلسله‌مراتبی تجمعی)

#### ۵.۲.۱ ایدهٔ اصلی

در HAC از هر نمونه یک خوشهٔ تک‌عضوی شروع می‌شود و در هر مرحله نزدیک‌ترین دو خوشه ادغام می‌شوند تا در نهایت تنها یک خوشه باقی بماند. تفاوت روش‌ها در تعریف **فاصله بین خوشه‌ها** است:

- **نزدیک‌ترین زوج (Single Linkage):** فاصلهٔ دو خوشه، کمترین فاصلهٔ بین هر جفت نقطه از دو خوشه است.
- **دورترین زوج (Complete Linkage):** فاصلهٔ دو خوشه، بیشترین فاصلهٔ بین هر جفت نقطه از دو خوشه است.

به دلیل پیچیدگی محاسباتی \(O(n^2)\) و بالاتر، HAC روی یک زیرمجموعهٔ تصادفی (مثلاً ۳۰۰ نمونه) از داده اجرا شده است.

#### ۵.۲.۲ پیاده‌سازی خلاصه

ابتدا ماتریس فاصلهٔ اقلیدسی بین همهٔ نمونه‌ها محاسبه می‌شود. سپس در هر مرحله:

- نزدیک‌ترین دو خوشه در ماتریس فاصله پیدا می‌شوند.
- این دو خوشه ادغام می‌شوند و خوشهٔ جدید جایگزین می‌شود.
- فاصلهٔ خوشهٔ جدید با سایر خوشه‌ها بسته به نوع لینک (حداقل یا حداکثر فاصله) به‌روزرسانی می‌شود.
- پس از هر ادغام، SSE افراز فعلی (مجموع SSE تمام خوشه‌ها) محاسبه و همراه با تعداد خوشه‌ها ذخیره می‌شود.

نمونه‌ای از کد محاسبهٔ SSE و به‌روزرسانی ساختار خوشه‌ها:

```python
def compute_sse_for_clusters(X, clusters):
    sse = 0.0
    for idx in clusters:
        points = X[idx]
        center = points.mean(axis=0)
        diff = points - center
        sse += np.sum(diff ** 2)
    return sse
```

تابع HAC برای هر دو حالت `linkage="single"` و `linkage="complete"` اجرا شده و برای Kهای مختلف مقدار SSE به‌دست آمده است.

---

## ۶. معیارها و نتایج

### ۶.۱ معیار تابع هزینه (SSE)

معیار اصلی استفاده‌شده برای ارزیابی کیفیت خوشه‌بندی، **SSE (Sum of Squared Errors)** است:

- هر چه SSE کمتر باشد، نقاط درون هر خوشه به مرکز آن نزدیک‌تر هستند.
- برای مقایسهٔ روش‌ها، SSE بر حسب K برای هر الگوریتم محاسبه شده است.

### ۶.۲ نتایج K-means

- با افزایش K، مقدار SSE کاهش می‌یابد.
- نمودار SSE بر حسب K شکل آرنجی دارد و می‌توان ناحیه‌ای را که کاهش SSE بعد از آن کند می‌شود به‌عنوان محدودهٔ مناسب K در نظر گرفت.
- نمودار تعداد تکرار تا همگرایی نشان می‌دهد که برای برخی مقادیر K، الگوریتم به سرعت همگرا می‌شود و برای برخی دیگر نیاز به تکرارهای بیشتری دارد.

### ۶.۳ نتایج HAC

- در HAC نیز با کاهش تعداد خوشه‌ها (از n به ۱)، SSE به صورت افزایشی روی درخت ادغام‌ها ثبت می‌شود و می‌توان آن را به تابع هزینه بر حسب K تبدیل کرد.
- تفاوت اصلی بین Single و Complete Linkage در نحوه ادغام خوشه‌ها و شکل نهایی خوشه‌ها است؛ این تفاوت در منحنی SSE نیز قابل مشاهده است.

---

## ۷. تحلیل و مقایسه روش‌ها

برای مقایسهٔ منصفانه، الگوریتم K-means روی همان زیرمجموعه‌ای از داده که HAC روی آن اجرا شده اعمال شده است. سپس برای Kهای مشترک (مثلاً ۲ تا ۱۰)، SSE سه روش زیر روی یک نمودار رسم شده است:

- K-means
- HAC با نزدیک‌ترین زوج (Single Linkage)
- HAC با دورترین زوج (Complete Linkage)

**نمودار ۲ — مقایسه تابع هزینه K-means و HAC**

![K-means vs HAC cost comparison](images/02.png)

**برداشت‌ها:**

- با افزایش K، انتظار می‌رود SSE در هر سه روش کاهش یابد، چون خوشه‌ها ریزتر می‌شوند و نقاط به مراکز نزدیک‌تر می‌شوند.
- منحنی K-means معمولاً نرم‌تر است و برای استفاده از روش آرنج مناسب‌تر به نظر می‌رسد.
- در HAC، بسته به نوع لینک، شکل منحنی می‌تواند متفاوت باشد:
  - Single Linkage تمایل به ساخت خوشه‌های زنجیره‌ای و کشیده دارد و می‌تواند نسبت به نویز حساس باشد.
  - Complete Linkage خوشه‌های فشرده‌تر ایجاد می‌کند و به نقاط دورتر حساس‌تر است.

با مقایسهٔ این منحنی‌ها می‌توان بازهٔ مناسبی برای K انتخاب و دربارهٔ این که کدام روش خوشه‌هایی با تفسیر بهتر برای این مجموعه‌داده تولید می‌کند، بحث کرد.

---

## ۸. نتیجه‌گیری

در این پروژه، خوشه‌بندی مشتریان بر اساس ویژگی‌های رفتاری و خرید با دو الگوریتم **K-means** و **HAC** انجام شد. نتایج نشان داد:

- K-means با هزینهٔ محاسباتی کمتر و پیاده‌سازی ساده، برای داده‌های با ابعاد متوسط گزینهٔ مناسبی است و منحنی SSE آن ابزار خوبی برای انتخاب K به‌کمک روش آرنج فراهم می‌کند.
- HAC با ارائهٔ ساختار سلسله‌مراتبی خوشه‌ها امکان تحلیل عمیق‌تری از روابط بین خوشه‌ها می‌دهد و با انتخاب نوع لینک می‌توان به خوشه‌هایی با شکل و تراکم متفاوت رسید.
- مقایسهٔ منحنی‌های SSE برای Kهای مختلف نشان می‌دهد که هر دو روش می‌توانند برای تحلیل رفتار مشتریان مفید باشند، اما انتخاب نهایی به معیارهای تفسیرپذیری، پیچیدگی محاسباتی و نیاز کاربردی بستگی دارد.

---

## ۹. فایل‌های پروژه

| فایل / پوشه | توضیح |
|-------------|--------|
| `04-05-Kmeans-HAC.ipynb` | نوت‌بوک اصلی خوشه‌بندی (K-means و HAC) |
| `04-05-Kmeans-HAC.html` | خروجی HTML اکسپورت‌شده از نوت‌بوک |
| `data/Customer marketing.csv` | فایل دادهٔ اصلی مشتریان |
| `/images` | عکس نمودارها  |
